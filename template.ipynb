{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing the Impact of Climate and Regional Information on Power Outages\n",
    "\n",
    "**Name**: Sweekrit Bhatnagar\n",
    "\n",
    "**Website Link**: [https://sweekrit-b.github.io/powerOutageML/](https://sweekrit-b.github.io/powerOutageML/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "from dsc80_utils import * # Feel free to uncomment and use this.\n",
    "\n",
    "# Import basic libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "# Import plotting libraries\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "pd.options.plotting.backend = 'plotly' # Set pandas plotting backend to plotly\n",
    "\n",
    "# Import ML libraries\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of dataset\n",
    "\n",
    "In this project, I analyze major outage events witnessed in the continental US from January 2000 to July 2016, where a \"major outage\" refers to an outage that impacted at least 50,000 customers or caused a firm load loss of 300 MW. In addition to the outages' characteristics, the dataset also contains information regarding regional climate information, land-use, electricity consumption patterns, and economic characteristics. \n",
    "\n",
    "I chose this dataset because it yields itself to interesting analysis on variables that predict outage, especially considering the diversity and amount of geographic, climate, and economic information available. Furthermore, the dataset itself is important due to the **real world consequences** of power outages - these events have major societal impact and affect key infrastructure, including hospitals, transport systems, communication networks, etc. Working with this dataset allows us to do important data analysis on the causes of these events, and inform policymaking and mitigation strategies in order to product cities and homes from major power outages.\n",
    "\n",
    "### Questions brainstorm and selection\n",
    "\n",
    "While looking at the dataset, I thought of the following questions that I could analyze:\n",
    "1. What is the background regional and climate information (i.e. the environment) of major power outages of varying severity and cause?\n",
    "2. How do socioeconomic conditions of the region affect the severity of outages as measured by the duration and number of people affected?\n",
    "3. Is there a distinct geographic association with coastal regions and power outage cause and/or severity? What background characteristics play into the distinction?\n",
    "\n",
    "However, I ultimately chose to go with the first question: \n",
    "**What is the background regional and climate information (i.e. the environment) of major power outages of varying severity and cause?**\n",
    "\n",
    "Specifically, I plan to look into how various features that can affect regional climate (such as the geographic region, the month the outage occurred, the presence of a hurricane, etc.) play into the impact characteristics of outages, focusing on outage duration and the cause category.\n",
    "\n",
    "Reasons why this question is important:\n",
    "1. Different climate patterns can strongly influence storm severity, equipment stress, grid load, etc. and can be key preidctors about whether an area is affected by a major power outage.\n",
    "2. Climate change sees an increased variance of climate events - being able to predict power outages quickly before they happened can be important to save infrastucture in times of climate disaster (ex. hurricanes or major storms).\n",
    "\n",
    "### Description of key columns and overall shape of dataset\n",
    "\n",
    "There are **1534 rows** in the dataset, indicating the presence of 1534 major outage events that occurred in the time period of the study, of which we find the following features relevant:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Category</th>\n",
    "      <th>Variable</th>\n",
    "      <th>Description</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr><td rowspan=\"5\">General Information</td><td>YEAR</td><td>Year when the outage event occurred</td></tr>\n",
    "    <tr><td>MONTH</td><td>Month when the outage event occurred</td></tr>\n",
    "    <tr><td>U.S._STATE</td><td>U.S. state where the outage occurred</td></tr>\n",
    "    <tr><td>POSTAL.CODE</td><td>Postal code of the U.S. state</td></tr>\n",
    "    <tr><td>NERC.REGION</td><td>NERC region involved in the outage</td></tr>\n",
    "    <tr><td rowspan=\"3\">Regional Climate Information</td><td>CLIMATE.REGION</td><td>U.S. climate region as defined by the National Centers for Environmental Information (9 regions total)</td></tr>\n",
    "    <tr><td>ANOMALY.LEVEL</td><td>Oceanic Niño/La Niña index (ONI), 3-month running mean of SST anomalies</td></tr>\n",
    "    <tr><td>CLIMATE.CATEGORY</td><td>Climate category (“Warm,” “Cold,” or “Normal”) based on ONI index ±0.5°C</td></tr>\n",
    "    <tr><td rowspan=\"10\">Outage Event Information</td><td>OUTAGE.START.DATE</td><td>Calendar day when the outage started</td></tr>\n",
    "    <tr><td>OUTAGE.START.TIME</td><td>Time of day when the outage started</td></tr>\n",
    "    <tr><td>OUTAGE.RESTORATION.DATE</td><td>Calendar day when power was fully restored</td></tr>\n",
    "    <tr><td>OUTAGE.RESTORATION.TIME</td><td>Time of day when power was fully restored</td></tr>\n",
    "    <tr><td>CAUSE.CATEGORY</td><td>High-level category describing the cause of the outage</td></tr>\n",
    "    <tr><td>CAUSE.CATEGORY.DETAIL</td><td>Detailed description of the event cause</td></tr>\n",
    "    <tr><td>HURRICANE.NAMES</td><td>Hurricane name if the outage was caused by a hurricane</td></tr>\n",
    "    <tr><td>OUTAGE.DURATION</td><td>Duration of the outage in minutes</td></tr>\n",
    "    <tr><td>DEMAND.LOSS.MW</td><td>Peak demand lost during the outage (megawatts)</td></tr>\n",
    "    <tr><td>CUSTOMERS.AFFECTED</td><td>Number of customers impacted by the outage</td></tr>\n",
    "    <tr><td rowspan=\"11\">Regional Land-Use Characteristics</td><td>POPULATION</td><td>Population of the U.S. state in the given year</td></tr>\n",
    "    <tr><td>POPPCT_URBAN</td><td>Percentage of the population living in urban areas</td></tr>\n",
    "    <tr><td>POPPCT_UC</td><td>Percentage of the population living in urban clusters</td></tr>\n",
    "    <tr><td>POPDEN_URBAN</td><td>Population density of urban areas (persons/sq. mile)</td></tr>\n",
    "    <tr><td>POPDEN_UC</td><td>Population density of urban clusters (persons/sq. mile)</td></tr>\n",
    "    <tr><td>POPDEN_RURAL</td><td>Population density of rural areas (persons/sq. mile)</td></tr>\n",
    "    <tr><td>AREAPCT_URBAN</td><td>Percentage of the state’s land area classified as urban</td></tr>\n",
    "    <tr><td>AREAPCT_UC</td><td>Percentage of the state’s land area classified as urban clusters</td></tr>\n",
    "    <tr><td>PCT_LAND</td><td>Percentage of total U.S. land area represented by the state</td></tr>\n",
    "    <tr><td>PCT_WATER_TOT</td><td>Percentage of total U.S. water area represented by the state</td></tr>\n",
    "    <tr><td>PCT_WATER_INLAND</td><td>Percentage of total U.S. inland water area represented by the state</td></tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data for analysis - outage is data is marked as cleaned because I removed irrelevant rows at the top of the Excel sheet that contained study information.\n",
    "# Note, I did not do any other cleaning here; it is all part of the project work.\n",
    "data = pd.read_csv('outage_cleaned.csv')\n",
    "# Determine the number of rows in the dataset\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the dataset, there are three primary changes to be made in order to effectively clean the data. Note that in this step, I will NOT be imputing or dealing with `NaN` values, as that will be important for future missingness analysis. \n",
    "1. Set the index of the dataset using a unique identifier instead of sticking with the default sequence.\n",
    "2. Perform datetime conversions. There seem to be separate columns for the calendar date and the time - combining these would make future analysis easier, and converting to datetime would allow for them to become quantitative discrete variables that I can use in future analysis if I choose to.\n",
    "3. Drop any unnecessary and/or redundant columns. Although in Steps 3-8 I am already filtering for the columns that I want, this will make my data cleaner and more easily perform the univariate and bi-variate analysis of this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1 - set the observation number to be the index of the dataset\n",
    "data = data.set_index('OBS')\n",
    "\n",
    "# Step 2 - datetime conversions. \n",
    "# Step 2.1 - Convert the OUTAGE.START.DATE and OUTAGE.START.TIME to datetime\n",
    "data['OUTAGE.START.DATE'] = pd.to_datetime(data['OUTAGE.START.DATE'] + ' ' + data['OUTAGE.START.TIME'], format='mixed')\n",
    "# Step 2.2 - Convert the OUTAGE.RESTORATION.DATE and OUTAGE.RESTORATION.TIME to datetime\n",
    "data['OUTAGE.RESTORATION.DATE'] = pd.to_datetime(data['OUTAGE.RESTORATION.DATE'] + ' ' + data['OUTAGE.RESTORATION.TIME'], format='mixed')\n",
    "\n",
    "# Step 3- drop redundant/unnecessary columns\n",
    "# Drop the OUTAGE.START.TIME and OUTAGE.RESTORATION.TIME columns\n",
    "data = data.drop(columns=['OUTAGE.START.TIME', 'OUTAGE.RESTORATION.TIME'])\n",
    "# Drop the US state column as it is redundant\n",
    "data = data.drop(columns=['U.S._STATE'])\n",
    "\n",
    "# Step 4 - replace 0 values in impact columns with NaN\n",
    "impact_columns = ['CUSTOMERS.AFFECTED', 'DEMAND.LOSS.MW']\n",
    "for col in impact_columns:\n",
    "    data[col] = data[col].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, I need to perform univariate and bivariate analysis. I wanted to create general functions to plot and save graphs and grab relevant ones in the notebook when I needed to. I defined the following functions:\n",
    "1. `plot_against_categorical_var` plots a boxplot of a categorical variables (such as the U.S. climate region) against a numerical metric (such as the outage duration).\n",
    "2. `plot_against_numerical_var` plots a scatter plot of a numerical variable (such as the anomaly level of El Nino or La Nina) against another numerical metric (such as the amount of customers affected).\n",
    "3. `plot_bivariate` uses the basis that the above two functions formed to create a figure with three subplots (and therefore perform an all-encompassing bivariate analysis) for a specified column in the dataset. Specifically, as defined in the problem statement above, I want to see how various climate-based factors affect outage impact factors, so for each column, I made a subplot comparing it to outage duration, demand loss (in Megawatts), and the customers affected.\n",
    "4. `plot_univariate` plots a univariate analysis of a column - if the data was categorical (such as the U.S. climate region), a histogram was plotted to demonstrate the category density. If the data was numerical, then the Seaborn library was used in conjunction with Matplotlib to create a KDE plot.\n",
    "5. `save_figure` saves an image to a folder for easy access and usage.\n",
    "6. `save_combined_univariate` plots a univariate analysis of all of the column in the dataset and creates a figure with all of them at once.\n",
    "\n",
    "Note that for a lot of these graphs, I noticed that outlier values, especially those in the impact metrics, were making the values difficult to interpret. Therefore, even though I did not remove them from the data, for the purpose of understanding the patterns present in the data better, I included an `exclude_top_pct` variable. Please note that this DOES NOT impact any future analysis, all it does is reveal patterns in graphs that would normally just look like uninterpretable lines with a few plotted outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a boxplot of a numerical column against a categorical variable, excluding the top X% of values\n",
    "def plot_against_categorical_var(categorical_var_col, column_name, exclude_top_pct=0.05):\n",
    "    threshold = data[column_name].quantile(1 - exclude_top_pct)  # grab the threshold value for top X%\n",
    "    filtered = data[data[column_name] <= threshold] # filter data to exclude top X%\n",
    "    title = f'{column_name} vs {categorical_var_col} (excluding top {exclude_top_pct*100:.0f}% values)' # title for the plot\n",
    "    fig = px.box(filtered, x=categorical_var_col, y=column_name, title=title) # use plotly express to create boxplot\n",
    "    fig.show() # show the figure\n",
    "\n",
    "# Plot a scatterplot of a numerical column against another numerical variable, excluding the top X% of values\n",
    "def plot_against_numerical_var(numerical_var_col, column_name, exclude_top_pct=0.05):\n",
    "    threshold = data[column_name].quantile(1 - exclude_top_pct) # grab the threshold value for top X%\n",
    "    filtered = data[data[column_name] <= threshold] # filter data to exclude top X%\n",
    "    title = f'{column_name} vs {numerical_var_col} (excluding top {exclude_top_pct*100:.0f}% values)' # title for the plot\n",
    "    fig = px.scatter(filtered, x=numerical_var_col, y=column_name, trendline='ols', trendline_color_override='red', title=title) # use plotly express to create scatterplot\n",
    "    fig.show() # show the figure\n",
    "\n",
    "# Plot bivariate analysis of a column against three metrics using subplots for a combined analysis\n",
    "def plot_bivariate(column, categorical = True, exclude_top_pct=0.05, save_folder=\"bivariate_analysis\", df=data):\n",
    "    metrics = ['OUTAGE.DURATION', 'DEMAND.LOSS.MW', 'CUSTOMERS.AFFECTED'] # outage impact metrics to plot against\n",
    "    subplot_titles = metrics # title variable for subplots\n",
    "\n",
    "    os.makedirs(save_folder, exist_ok=True) # ensure folder that plots are being saved to exists\n",
    "    print(f\"Plotting {column}...\") # indicate start of plotting for debugging\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=subplot_titles) # create subplots with 1 row and 3 columns\n",
    "    \n",
    "    for i, metric in enumerate(metrics): # iterate over metrics\n",
    "        # Filter out top fraction using the same logic as above plotting functions\n",
    "        threshold = df[metric].quantile(1 - exclude_top_pct) # calculate threshold for top X%\n",
    "        filtered = df[df[metric] <= threshold] # filter data to exclude top X%\n",
    "        \n",
    "        if categorical: # if the column is defined as categorical in the function call\n",
    "            # Boxplot for categorical x-axis\n",
    "            for category in filtered[column].unique(): # iterate over unique categories\n",
    "                fig.add_trace( # add boxplot trace for each category\n",
    "                    go.Box(\n",
    "                        y=filtered[filtered[column] == category][metric], # y values for the specific category within the column\n",
    "                        name=str(category), # name of the category\n",
    "                        boxmean='sd', # show mean and standard deviation\n",
    "                        showlegend=(i==0)  # show legend only for first subplot\n",
    "                    ),\n",
    "                    row=1, col=i+1 # specify row and column for subplot\n",
    "                )\n",
    "        else: # if the column is defined as numerical in the function call\n",
    "            # Scatterplot with line of best fit\n",
    "            scatter_fig = px.scatter(filtered, x=column, y=metric) # create scatterplot using plotly express\n",
    "            for trace in scatter_fig.data: # iterate over traces in scatterplot\n",
    "                fig.add_trace(trace, row=1, col=i+1) # add scatterplot trace to subplot\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=500, width=1800,\n",
    "        title_text=f\"Metrics vs {column} (excluding top {exclude_top_pct*100:.0f}% values)\"\n",
    "    ) # update layout of figure to be more readable\n",
    "    \n",
    "    print(f\"Finished plotting {column}.\") # indicate end of plotting\n",
    "    return fig # return the figure object\n",
    "\n",
    "# Plot univariate analysis of a column - histogram for categorical, KDE for numerical\n",
    "def plot_univariate(column_name, categorical=False, df=data):\n",
    "    if categorical: # if the column is defined as categorical in the function call\n",
    "        # Histogram/bar chart for categorical data\n",
    "        fig = go.Figure() # create empty figure\n",
    "        fig.add_trace( # add histogram trace\n",
    "            go.Histogram(\n",
    "                x=df[column_name],\n",
    "                name=f'{column_name} counts',\n",
    "            )\n",
    "        )\n",
    "        fig.update_layout( # update layout of figure\n",
    "            title_text=f'Histogram of {column_name}',\n",
    "            xaxis_title=column_name,\n",
    "            yaxis_title='Count',\n",
    "            width=600, height=400\n",
    "        )\n",
    "    else:\n",
    "        values = df[column_name].values # get values of the column\n",
    "        \n",
    "        kde_fig = sns.kdeplot(values, bw_method='scott') # create KDE plot using seaborn\n",
    "        kde_data = kde_fig.get_lines()[0].get_data() # get KDE data points\n",
    "        x_grid = kde_data[0] # x values of the KDE plot\n",
    "        kde_values = kde_data[1] # y values of the KDE plot\n",
    "        kde_fig.figure.clear()  # clear the seaborn figure for future plots\n",
    "\n",
    "        fig = go.Figure() # create empty figure\n",
    "        fig.add_trace( # add scatterplot trace from the values derived from the KDE\n",
    "            go.Scatter(\n",
    "                x=x_grid,\n",
    "                y=kde_values,\n",
    "                fill='tozeroy',\n",
    "                name='KDE'\n",
    "            )\n",
    "        )\n",
    "        fig.update_layout( # update layout of figure\n",
    "            width=600, height=400,\n",
    "            title_text=f'KDE of {column_name}',\n",
    "            xaxis_title=column_name,\n",
    "            yaxis_title='Density',\n",
    "            showlegend=False\n",
    "        )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Save figure to specified folder\n",
    "def save_figure(fig, column_name, save_folder):\n",
    "    os.makedirs(save_folder, exist_ok=True) # ensure folder exists\n",
    "    save_path = os.path.join(save_folder, f\"{column_name}.png\") # construct save path\n",
    "    fig.write_image(save_path) # save figure as PNG\n",
    "    print(f\"Saved figure to {save_path}\") # indicate where figure was saved\n",
    "\n",
    "# Create a combined univariate analysis figure for multiple columns\n",
    "def save_combined_univariate(columns_to_plot, save_path=\"univariate_analysis/combined.png\", cols_per_row=5):\n",
    "    n_cols = cols_per_row # number of columns per row\n",
    "    n_rows = math.ceil(len(columns_to_plot) / n_cols) # calculate number of rows needed\n",
    "    \n",
    "    # Create subplot titles\n",
    "    subplot_titles = [col for col, _ in columns_to_plot]\n",
    "    \n",
    "    fig = make_subplots(rows=n_rows, cols=n_cols, subplot_titles=subplot_titles) # create a figure with subplots\n",
    "    \n",
    "    for i, (column_name, categorical) in enumerate(columns_to_plot): # iterate over columns to plot\n",
    "        row = i // n_cols + 1 # calculate row index\n",
    "        col = i % n_cols + 1 # calculate column index\n",
    "        \n",
    "        # Create figure for this column\n",
    "        col_fig = plot_univariate(column_name, categorical=categorical)\n",
    "        \n",
    "        # Add traces to the combined figure\n",
    "        for trace in col_fig.data:\n",
    "            fig.add_trace(trace, row=row, col=col)\n",
    "        \n",
    "        # Adjust x-axis and y-axis titles for each subplot\n",
    "        fig.update_xaxes(title_text=column_name, row=row, col=col)\n",
    "        fig.update_yaxes(title_text='Count' if categorical else 'Density', row=row, col=col)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=n_rows * 400,\n",
    "        width=n_cols * 600,\n",
    "        title_text=\"Combined Univariate Analysis\",\n",
    "        showlegend=False\n",
    "    ) # update layout of the combined figure to be more readable\n",
    "    \n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True) # ensure save directory exists\n",
    "    fig.write_image(save_path) # save the combined figure as PNG\n",
    "    print(f\"Saved combined figure to {save_path}\") # indicate where figure was saved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns to plot in a list of tuples (column_name, is_categorical)\n",
    "columns_to_plot = [\n",
    "    # Time of year\n",
    "    ('MONTH', True),\n",
    "\n",
    "    # Geographic region information\n",
    "    ('NERC.REGION', True),\n",
    "    ('POSTAL.CODE', True),\n",
    "\n",
    "    # Regional Climate Information\n",
    "    ('CLIMATE.REGION', True),\n",
    "    ('CLIMATE.CATEGORY', True),\n",
    "    ('ANOMALY.LEVEL', False),\n",
    "\n",
    "    # Event causes\n",
    "    ('CAUSE.CATEGORY', True),\n",
    "    ('HURRICANE.NAMES', True),\n",
    "\n",
    "    # Electricity price information\n",
    "    ('RES.PRICE', False),\n",
    "    ('COM.PRICE', False),\n",
    "    ('IND.PRICE', False),\n",
    "\n",
    "    # Electricity consumption information\n",
    "    ('RES.SALES', False),\n",
    "    ('COM.SALES', False),\n",
    "    ('IND.SALES', False),\n",
    "    ('RES.PERCEN', False),\n",
    "    ('COM.PERCEN', False),\n",
    "    ('IND.PERCEN', False),\n",
    "\n",
    "    # Customers served information\n",
    "    ('RES.CUSTOMERS', False),\n",
    "    ('COM.CUSTOMERS', False),\n",
    "    ('IND.CUSTOMERS', False),\n",
    "\n",
    "    # Regional economic output information\n",
    "    ('PC.REALGSP.STATE', False),\n",
    "    ('PC.REALGSP.USA', False),\n",
    "    ('PC.REALGSP.REL', False),\n",
    "    ('PC.REALGSP.CHANGE', False),\n",
    "    ('UTIL.REALGSP', False),\n",
    "    ('TOTAL.REALGSP', False),\n",
    "    ('UTIL.CONTRI', False),\n",
    "    ('PI.UTIL.OFUSA', False),\n",
    "\n",
    "    # Population information\n",
    "    ('POPULATION', False),\n",
    "    ('POPPCT_URBAN', False),\n",
    "    ('POPPCT_UC', False),\n",
    "    ('POPDEN_URBAN', False),\n",
    "    ('POPDEN_UC', False),\n",
    "    ('POPDEN_RURAL', False),\n",
    "\n",
    "    # Land information\n",
    "    ('AREAPCT_URBAN', False),\n",
    "    ('AREAPCT_UC', False),\n",
    "    ('PCT_LAND', False),\n",
    "    ('PCT_WATER_TOT', False),\n",
    "    ('PCT_WATER_INLAND', False)\n",
    "]\n",
    "\n",
    "# Specify output columns to plot\n",
    "output_columns = [('OUTAGE.DURATION', False), ('DEMAND.LOSS.MW', False), ('CUSTOMERS.AFFECTED', False)]\n",
    "\n",
    "# Plot and save bivariate and univariate analyses for specified columns\n",
    "for col, is_categorical in columns_to_plot:\n",
    "    fig1 = plot_bivariate(col, categorical=is_categorical) # create the figure for bivariate analysis\n",
    "    fig2 = plot_univariate(col, categorical=is_categorical) # create the figure for univariate analysis\n",
    "    save_figure(fig1, col, save_folder=\"bivariate_analysis\") # save the bivariate figure\n",
    "    save_figure(fig2, col, save_folder=\"univariate_analysis\")  # save the univariate figure\n",
    "\n",
    "for col, is_categorical in output_columns:\n",
    "    fig = plot_univariate(col, categorical=is_categorical) # create the figure for univariate analysis\n",
    "    save_figure(fig, col, save_folder=\"univariate_analysis\") # save the univariate figure\n",
    "\n",
    "save_combined_univariate(columns_to_plot + output_columns, save_path=\"univariate_analysis/all_univariate.png\", cols_per_row=5) # save combined univariate figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to showcase the graphs as part of the analysis, I have uploaded some relevant plots here and describe/interpret the trends that are present. Note that these graphs were generated using the above functions and saved in a separate folder, which I then pulled from to place these images in the Jupyter Notebook.\n",
    "\n",
    "### Bivariate analysis\n",
    "\n",
    "#### Climate region\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"bivariate_analysis/CLIMATE.REGION.png\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "We can make the following notes about this graph:\n",
    "1. The East North Central, although having the largest mean and median outage duration, does not have a significantly greater demand lost or customers affected.\n",
    "2. The Northeast region seems to have significant variation in both the outage duration and the amount of customers affected, but relatively standard variance (as compared to other columns) for the demand lost.\n",
    "3. Considering we removed the top 5% of values, we find that for outage duration, demand lost, and customers affected, a majority of the values tend to be on the lower end of the range of values in the column. This implies that outliers may become an issue if I choose to do regression-based predictions of these values in later parts of the project.\n",
    "\n",
    "#### Climate category\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"bivariate_analysis/CLIMATE.CATEGORY.png\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "We can see that there seems to be no significant difference between any of the impact metrics that we have defined and the category of climate (i.e. whether the year is warm, cold, or normal). This implies that there is no relationship with the severity of an outage and the temperature of that year. This is unexpected - I expected years with higher or lower than average temperatures to have greater outages, possibly due to increased severe weather events. \n",
    "\n",
    "\n",
    "#### Month\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"bivariate_analysis/MONTH.png\" width=\"800\">\n",
    "</div>\n",
    "\n",
    "We can make the following notes about this graph:\n",
    "1. At first glance, other than September (which we will talk about in the next point), there do not seem to be any obvious differences month by month for each of the impact metrics - outage duration shows the most variation by month, but common groupings (ex. seasonal) do not reveal any clear patterns.\n",
    "2. However, here seems to be a slightly larger mean outage duration and demand lost in September. This makes sense, considering that September is the peak of hurricane season, as showcased by the reference graph from NOAA. \n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"reference_imgs/hurricane_season_graph.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "### Univariate Analysis\n",
    "\n",
    "#### Outage metrics\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"univariate_analysis/OUTAGE.DURATION.png\" width=\"300\">\n",
    "    <img src=\"univariate_analysis/CUSTOMERS.AFFECTED.png\" width=\"300\">\n",
    "    <img src=\"univariate_analysis/DEMAND.LOSS.MW.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "The graphs above are the univariate analyses of the outage metrics that we had defined before, including the outage duration, customers affected, and demand loss by Megawatts. These graphs confirm what we theorized earlier in our bivariate analysis - that all of these columns are significantly right-skewed. This can cause problems in regression analysis if we were to try to predict these metrics.\n",
    "\n",
    "#### Climate features (from bivariate analysis)\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"univariate_analysis/CLIMATE.REGION.png\" width=\"300\">\n",
    "    <img src=\"univariate_analysis/CLIMATE.CATEGORY.png\" width=\"300\">\n",
    "    <img src=\"univariate_analysis/MONTH.png\" width=\"300\">\n",
    "</div>\n",
    "\n",
    "These graphs show the univariate analyses of the features we discussed in the bivariate analysis above. We can make the following observations:\n",
    "1. The Northeast region by far faces the largest amount of power outages, with over 300 of them, followed by the South and the West. Furthermore, the West North Central and Southwest regions face the least amount of power outages. This could imply that there are climate/regional characteristics for these regions that cause them to have more/less outages.\n",
    "2. The most power outages occur in normal years, then cold years, and finally in warm years. However, unlike the climate region, we cannot be quick to assume that regional/climate characteristics cause this. It is more likely that the \"normal\" climate category encompasses a much larger portion of the years studied because the average year **is** normal.\n",
    "3. The summer months seem to have the largest proportion of power outages, followed closely by the late winter months. This might be due to thunderstorms and tornadoes in the summer months and winter storms in the late winter. This can be verified by looking at the following reference graphs (News West 9, Fox 32, The Weather Channel).\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"reference_imgs/newswest_9_severe_weather_season.jpg\" width=\"300\">\n",
    "    <img src=\"reference_imgs/fox_32_tornados.webp\" width=\"300\">\n",
    "    <img src=\"reference_imgs/weather_channel_winter_storms.webp\" width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interesting Aggregates\n",
    "\n",
    "Following the univariate and bivariate analysis, I was interested in aggregating the cause category, climate region, and the mean/count of outages that occurred. This would give me insight into which combination of the two features would have the highest mean outage duration and highest number of outages. Not only could this allow me to define more specific and interesting hypothesis tests later on, but also gives insight into what causes high-impact outages in each region.\n",
    "\n",
    "To do this, I defined the function `pivot_and_heatmap`, which upon taking in a dataframe, values for the index, columns, and values, and the aggregation function, created a pivot table and then a heatmap  of the aggregation. Once again, as with the univariate and bivariate analysis, I saved these images in a folder. However, I display them after the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved figure to heatmap_analysis\\heatmap_mean_outage_duration.png\n",
      "Saved figure to heatmap_analysis\\heatmap_count_outage_duration.png\n"
     ]
    }
   ],
   "source": [
    "# Create pivot table and heatmap from the data given the indices, columns, values, and aggregation function\n",
    "def pivot_and_heatmap(df, index_col, columns_col, values_col, aggfunc='mean', exclude_top_pct=0.10): # function to create pivot table and heatmap\n",
    "    threshold = df[values_col].quantile(1 - exclude_top_pct) # calculate threshold for top X%\n",
    "    filtered = df[df[values_col] <= threshold] # filter data to exclude top X%\n",
    "    pivot_table = filtered.pivot_table(index=index_col, columns=columns_col, values=values_col, aggfunc=aggfunc) # create pivot table\n",
    "    pivot_table.fillna(0, inplace=True) # fill NaN values with 0\n",
    "    fig = px.imshow( # create heatmap using plotly express\n",
    "        pivot_table,\n",
    "        labels=dict(x=columns_col, y=index_col, color=values_col),\n",
    "        x=pivot_table.columns,\n",
    "        y=pivot_table.index,\n",
    "        text_auto=True,\n",
    "        aspect='auto',\n",
    "        color_continuous_scale='Viridis'\n",
    "    )\n",
    "    fig.update_layout( # update layout of figure\n",
    "        title=f'{values_col} {aggfunc} by {index_col} and {columns_col}, excluding top {exclude_top_pct*100:.0f}%',\n",
    "        xaxis_title=columns_col,\n",
    "        yaxis_title=index_col,\n",
    "        height=600,\n",
    "        width=900\n",
    "    )\n",
    "    return fig # return the heatmap figure\n",
    "\n",
    "save_figure( # plot and save heatmap of mean outage duration by cause category and climate region\n",
    "    pivot_and_heatmap(data, index_col='CAUSE.CATEGORY', columns_col='CLIMATE.REGION', values_col='OUTAGE.DURATION', aggfunc='mean', exclude_top_pct=0.10),\n",
    "    'heatmap_mean_outage_duration',\n",
    "    save_folder='heatmap_analysis'\n",
    ")\n",
    "\n",
    "save_figure( # plot and save heatmap of count of outages by cause category and climate region\n",
    "    pivot_and_heatmap(data, index_col='CAUSE.CATEGORY', columns_col='CLIMATE.REGION', values_col='OUTAGE.DURATION', aggfunc='count', exclude_top_pct=0),\n",
    "    'heatmap_count_outage_duration',\n",
    "    save_folder='heatmap_analysis'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was able to get the following graphs from my aggregate analysis:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"heatmap_analysis/heatmap_count_outage_duration.png\" width=\"500\">\n",
    "    <img src=\"heatmap_analysis/heatmap_mean_outage_duration.png\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "We can gain some interesting insights from this.\n",
    "1. The most outages seem to happen in the Northeast and in severe weather, with Northeastern severe weather outages being the most common power outage aggregate by far. This can imply that the Northeastern region faces significantly harsher climate conditions, as our bivariate analysis before also revealed its uniqueness in the severity of its climate impact metrics. However, it also has the highest rate of intentional attacks by a significant margin, implying that there might be infrastructure issues in the Northeast causing power outages.\n",
    "2. The highest mean outage duration seems to be for severe weather events in the Northwest and East North Central region, implying that those regions have a more difficult time restoring power after severe weather events. However, we also note that severe weather as a whole has some of the highest outage durations on a per-region basis, implying that each region might be uniquely ill-equipped to deal with severe weather outages.\n",
    "\n",
    "Overall, we are able to get a variety of interesting insights through the graphs and many possible hypotheses that we can explore. It is important to note that although I extrapolate a lot of possibilities from the graphs drawn, **none of them are confirmed yet**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this missingness analysis, I chose to focus on the `OUTAGE.DURATION` column - I was interested in analyzing whether this column's missing values were MAR (missing at random) and therefore dependent on any other \"feature columns\". Specifically, in my analysis I tried to determine if `OUTAGE.DURATION` was MAR or not MAR when compared against `CAUSE.CATEGORY` and `MONTH`. To perform my analysis most efficiently, I chose to define a variety of helper functions:\n",
    "1. `get_necessary_columns` retrieves relevant columns from the data to make calling on the data within functions and debugging easier.\n",
    "2. `shuffle_missing` is the shuffling function used to perform the permutation test between null and not-null data.\n",
    "3. `calculate_categorical_proportion` determined the proportion of each category in a column to determine the TVD between null and not-null data.\n",
    "4. `calculate_tvd` is the function to calculate the TVD between two distributions of categorical data.\n",
    "5. `calculate_tvd_between_nulls` is a function that filters, calculates categorical proportions, and then calculates the TVD between null and not null values.\n",
    "6. `perform_missingness_permutation_test` is a performs the permutation test by combining all the previous functions and `n_repetitions`\n",
    "7. `calculate_p_value` calculates the p-value of a distribution and an observed statistic.\n",
    "8. `plot_permutation_test_distribution` uses a histogram to plot and visualize the permutation test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_counts = data.isnull().sum() # count missing values per column in a series\n",
    "missing_counts = missing_counts[missing_counts > 0].sort_values(ascending=False) # filter to only columns with missing values and sort descending\n",
    "\n",
    "def get_necessary_columns(data, col_list): # function to get necessary columns from data\n",
    "    return data[col_list]\n",
    "\n",
    "def shuffle_missing(data, shuffle_column): # function to shuffle a column in the data\n",
    "    data_shuffled = data.assign(shuffled = np.random.permutation(data[shuffle_column])) # create new column with shuffled values\n",
    "    return data_shuffled\n",
    "\n",
    "def calculate_categorical_proportions(data, categorical_column, target_column): # function to calculate proportions of categorical variable\n",
    "    groupby_filtered = data.groupby(categorical_column)[target_column].size().fillna(0) # group by categorical column and count target column\n",
    "    groupby_filtered = groupby_filtered / groupby_filtered.sum() # normalize to get proportions\n",
    "    return groupby_filtered \n",
    "\n",
    "def calculate_tvd(proportions1, proportions2): # function to calculate total variation distance between two distributions\n",
    "    return (proportions1 - proportions2).abs().sum() / 2\n",
    "\n",
    "def calculate_tvd_between_nulls(data, categorical_column, target_column): # function to calculate TVD between null and non-null distributions\n",
    "    data_not_null = data[data[target_column].notnull()] # filter data to only non-null target column values\n",
    "    groupby_not_null = calculate_categorical_proportions(data_not_null, categorical_column, target_column)\n",
    "    data_null = data[data[target_column].isnull()] # filter data to only null target column values\n",
    "    groupby_null = calculate_categorical_proportions(data_null, categorical_column, target_column)\n",
    "    tvd = calculate_tvd(groupby_not_null, groupby_null)\n",
    "    return tvd\n",
    "\n",
    "def perform_missingness_permutation_test(data, categorical_column, target_column, n_repetitions=1000): # function to perform permutation test for missingness\n",
    "    filtered_data = get_necessary_columns(data, [categorical_column, target_column]) # filter data to necessary columns\n",
    "    observed_value = calculate_tvd_between_nulls(filtered_data, categorical_column, target_column) # calculate observed TVD\n",
    "    tvds = [] # list to store TVDs from permutations\n",
    "\n",
    "    for _ in range(n_repetitions):  # perform permutations\n",
    "        shuffled_data = shuffle_missing(filtered_data, target_column) # shuffle target column to break association\n",
    "        tvd = calculate_tvd_between_nulls(shuffled_data, categorical_column, 'shuffled') # calculate TVD for shuffled data\n",
    "        tvds.append(tvd) # store TVD from this permutation\n",
    "    \n",
    "    return observed_value, tvds # return observed TVD and list of TVDs from permutations\n",
    "\n",
    "def calculate_p_value(observed_statistic, permuted_vals): # function to calculate p-value from permutation test\n",
    "    p_value = np.mean(np.array(permuted_vals) >= observed_statistic) # calculate p-value as proportion of permuted values >= observed statistic\n",
    "    return p_value\n",
    "\n",
    "def plot_permutation_test_distribution(permuted_vals, observed_statistic, categorical_column_name, target_column_name, p_value=None, title=None): # function to plot permutation test distribution\n",
    "    if title is None:\n",
    "        title = (\n",
    "            f'Empirical Distribution of the TVD of {categorical_column_name} Proportion Differences<br>'\n",
    "            f'Between Null and Not Null Values of {target_column_name}'\n",
    "        ) # title for the plot\n",
    "    fig = px.histogram(pd.DataFrame(permuted_vals), x=0, nbins=50, histnorm='probability', \n",
    "                        title=title, width=800, height=500,) # create histogram of permuted TVDs\n",
    "    fig.add_vline(x=observed_statistic, line_color='red', line_width=1, opacity=1) # add vertical line for observed statistic\n",
    "    fig.add_annotation(text=f'<span style=\"color:red\">Observed TVD = {round(observed_statistic, 2)}</span><br><span style=\"color:red\">P-value = {round(p_value, 3)}</span>',\n",
    "                   x=1.2 * observed_statistic, showarrow=False, y=0.05) # add annotation for observed statistic and p-value\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAR Dependence on Cause Category\n",
    "\n",
    "The following code performs the test checking if the missing values in outage duration depend on cause category. Specifically, the pair of hypotheses are:\n",
    "\n",
    "1. **Null**: The missingness of OUTAGE.DURATION is not MAR dependent on CAUSE.CATEGORY.\n",
    "2. **Alternative**: The missingness of OUTAGE.DURATION is MAR dependent on CAUSE.CATEGORY.\n",
    "3. **Test statistic**: The TVD of CAUSE.CATEGORY values for missing and non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.002\n",
      "Saved figure to hyp_perm_test_analysis\\missingness_permutation_test_cause_category_outage_duration.png\n"
     ]
    }
   ],
   "source": [
    "observed_statistic, permuted_vals = perform_missingness_permutation_test(data, 'CAUSE.CATEGORY', 'OUTAGE.DURATION', n_repetitions=1000) # perform permutation test\n",
    "p_value = calculate_p_value(observed_statistic, permuted_vals) # calculate p-value\n",
    "print(f'P-value: {p_value}')\n",
    "fig = plot_permutation_test_distribution(permuted_vals, observed_statistic, 'CAUSE.CATEGORY', 'OUTAGE.DURATION', p_value=p_value) # plot permutation test distribution\n",
    "save_figure(fig, 'missingness_permutation_test_cause_category_outage_duration', save_folder='hyp_perm_test_analysis') # save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the code above, we find that the p-value of this permutation test is less than 0.05, the significance threshold used in data science. This means that we **reject the null** and can therefore state that the missigness of OUTAGE.DURATION is MAR dependent on CAUSE.CATEGORY. Here is the plot from the permutation test:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"hyp_perm_test_analysis/missingness_permutation_test_cause_category_outage_duration.png\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAR Dependence on Month\n",
    "\n",
    "The following code performs the test checking if the missing values in outage duration depend on the outage month. Specifically, the pair of hypotheses are:\n",
    "\n",
    "1. **Null**: The missingness of OUTAGE.DURATION is not MAR dependent on MONTH.\n",
    "2. **Alternative**: The missingness of OUTAGE.DURATION is MAR dependent on MONTH.\n",
    "3. **Test statistic**: The TVD of MONTH values for missing and non-missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.092\n",
      "Saved figure to hyp_perm_test_analysis\\missingness_permutation_test_month_outage_duration.png\n"
     ]
    }
   ],
   "source": [
    "observed_statistic, permuted_vals = perform_missingness_permutation_test(data, 'MONTH', 'OUTAGE.DURATION', n_repetitions=1000) # perform permutation test\n",
    "p_value = calculate_p_value(observed_statistic, permuted_vals) # calculate p-value\n",
    "print(f'P-value: {p_value}')\n",
    "fig = plot_permutation_test_distribution(permuted_vals, observed_statistic, 'MONTH', 'OUTAGE.DURATION', p_value=p_value) # plot permutation test distribution\n",
    "save_figure(fig, 'missingness_permutation_test_month_outage_duration', save_folder='hyp_perm_test_analysis') # save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the code above, we find that the p-value of this permutation test is greater than 0.05, the significance threshold used in data science. This means that we **fail to reject the null** and can therefore state that the missigness of MONTH is not MAR dependent on CAUSE.CATEGORY. Here is the plot from the permutation test:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"hyp_perm_test_analysis/missingness_permutation_test_month_outage_duration.png\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the hypothesis test, I decided to test a pattern I observed in the bivariate analysis - the relationship with more Northern regions and increased OUTAGE.DURATION values. Specifically, my hypotheses and test statistic were:\n",
    "1. **Null**: The outage duration for regions classified as \"North\" is not greater than the outage duration for all regions (i.e. is consistent with random sampling). In other words, Northern regions are **not special**.\n",
    "2. **Alternative**: The outage duration for regions classified as \"North\" is greater than the outage duration for all regions.\n",
    "3. **Test statistic**: The mean of the outage duration for \"North\" regions.\n",
    "\n",
    "This was important for the question I am trying to answer which relates regional/climate information with the severity of outages because it establishes a clear relationship between geographically north (and usually colder) regions and a greater outage durations. \n",
    "1. My null and alternate hypotheses directly targets the question I have about regional/climate impact by packaging a variety of climate regions into what is essentially a binary variable, allowing for a low granularity but important test regaridng whether geographic location matters. My null hypotheses represents the **baseline assumption** that any observed difference in outages across large scale regional/climate categories is random variation.\n",
    "2. My test statistic, specifically just calculating the mean outage duration for randomly sampled/observed \"North\" regions, is also justifiable based on the way the p-value is calculated. Since we calculate the p-value by doing `np.mean(np.array(sampled_vals) >= observed_statistic)`, if the p-value is less than the significance threshold (0.05), then that means we reject the null hypothesis, and vice versa (i.e. we don't need to calculate differences due to the nature of our p-value calculation).\n",
    "3. The p-value threshold is the standard threshold for hypothesis testing. There is not a compelling reason to change this value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.001\n",
      "Saved figure to hyp_perm_test_analysis\\hypothesis_test_climate_region_binned_outage_duration.png\n"
     ]
    }
   ],
   "source": [
    "# Define a mapping from detailed climate regions to 'North' and 'Not North'\n",
    "climate_region_map = {'North': ['Northeast', 'East North Central', 'Northwest', 'West North Central'], 'Not North': ['South', 'West', 'Central', 'Southeast', 'Southwest']}\n",
    "# Reverse the map to easily map each region to its bin\n",
    "bin_map = {region: group for group, regions in climate_region_map.items() for region in regions}\n",
    "# Map the CLIMATE.REGION to the binned version\n",
    "data['CLIMATE.REGION.BINNED'] = data['CLIMATE.REGION'].map(bin_map)\n",
    "# Determine the counts in each bin for sampling later on\n",
    "binned_vals = data['CLIMATE.REGION.BINNED'].value_counts()\n",
    "# Determine the observed mean for the 'North' bin\n",
    "observed_mean = data.loc[data['CLIMATE.REGION.BINNED'] == 'North', 'OUTAGE.DURATION'].mean()\n",
    "# Perform permutation test\n",
    "n_repetitions = 1000\n",
    "permuted_means = []\n",
    "for _ in range(n_repetitions):\n",
    "    sample_mean = data['OUTAGE.DURATION'].sample(n=binned_vals['North']).mean()\n",
    "    permuted_means.append(sample_mean)\n",
    "# Calculate p-value\n",
    "p_value = calculate_p_value(observed_mean, permuted_means)\n",
    "print(f'P-value: {p_value}')\n",
    "# Plot the hypothesis test distribution\n",
    "title = (\n",
    "    f'Empirical Distribution of the Mean OUTAGE.DURATION<br>'\n",
    "    f'For Random Samples of Size {binned_vals[\"North\"]} from the Full Dataset'\n",
    ")\n",
    "fig = px.histogram(pd.DataFrame(permuted_means), x=0, nbins=50, histnorm='probability', \n",
    "                    title=title)\n",
    "fig.add_vline(x=observed_mean, line_color='red', line_width=1, opacity=1)\n",
    "fig.add_annotation(text=f'<span style=\"color:red\">Observed Mean for NORTH = {round(observed_mean, 2)}</span><br><span style=\"color:red\">P-value = {round(p_value, 3)}</span>',\n",
    "                   x=1.2 * observed_mean, showarrow=False, y=0.05)\n",
    "save_figure(fig, 'hypothesis_test_climate_region_binned_outage_duration', save_folder='hyp_perm_test_analysis') # save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that the p-value of this hypothesis test is indeed less than 0.05, causing us to **reject the null hypothesis** and demonstrating that Northern regions likely have a higher outage duration compared to the general data. Here is the plot for the hypothesis test:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"hyp_perm_test_analysis/hypothesis_test_climate_region_binned_outage_duration.png\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction problem that I am trying to solve is to predict the cause of a major power outage based on information about the climate, environment, and region of the outage.\n",
    "1. This is a **multiclass classification problem**, where each outage belongs to exactly one cause class.\n",
    "2. I am choosing **accuracy** as the classification metric over precision, recall, or F1-score:\n",
    "   1. While severe weather outages are more common, precision and recall are primarily meaningful in binary classification or when some classes are more critical. Here, all outage causes are equally important, and we want to correctly identify the cause regardless of class.\n",
    "   2. In multiclass classification, each instance belongs to only one class. Prioritizing precision for one class reduces false positives for that class but increases false negatives for it, which in turn increases false positives for other classes. Similarly, prioritizing recall for one class reduces its false negatives but increases its false positives, shifting errors to other classes. Therefore, precision and recall metrics are difficult to interpret in this zero-sum scenario.\n",
    "   3. F1-score addresses the precision-recall tradeoff but still requires averaging across classes and is less intuitive. Since all classes are equally important, accuracy is simpler and easier to interpret, and therefore is what I am using.\n",
    "   4. Overall, accuracy provides a clear, interpretable, and holistic measure of model performance for this multiclass task.\n",
    "3. The **response variable** I am using is the cause category, as it directly aligns with my research problem on investigating outage information (such as cause and severity) using regional/climate information. Furthermore, since causes of outages are discrete and mutually exclusive, it lends itself to a classification problem.\n",
    "   1. In terms of real world impact, predicting the cause allows stakeholders to take preventative actions depending on the type of outage, or make policy decisions regarding mitigative measures.\n",
    "4. My baseline model uses the following columns for prediction `['POSTAL.CODE', 'NERC.REGION', 'MONTH', 'YEAR', 'CLIMATE.REGION', 'ANOMALY.LEVEL', 'CLIMATE.CATEGORY', 'HURRICANE.NAMES', 'PCT_LAND', 'PCT_WATER_TOT', 'PCT_WATER_INLAND']` (and my final model uses a subset of these). If we are trying to predict the cause of an outage, all of these are pieces of infomration we would know beforehand - i.e. there is no feature on here that is directly impacted by the fact that there IS an outage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This baseline model is a **decision tree** uses a series of climate relevant models using the following columns:\n",
    "1. `POSTAL.CODE`: nominal data that provides us with the state.\n",
    "2. `NERC.REGION`: nominal data that provides us with the North American Electric Reliability Corporation regions.\n",
    "3. `MONTH`: quantiative data that indicates the month that the outage occurred.\n",
    "4. `YEAR`: quantiative data that indicates the year that the outage occurred.\n",
    "5. `CLIMATE.REGION`: nominal data that provides us with one of nine climatically consistent regions in the continental US.\n",
    "6. `ANOMALY.LEVEL`: quantitative data that provides us with the oceanic El Nino/La Nina index referring to the cold and warm episodes by season.\n",
    "7. `CLIMATE.CATEGORY`: ordinal data that categorizes climate episodes corresponding to years (cold, normal, and warm have an inherent ordering to them).\n",
    "8. `HURRICANE.NAMES`: nominal data that indicates the hurricane that was occurring at the time of the outage (if there was one)\n",
    "9. `PCT_LAND`: quantitative data incidating the percentage of the land area in the state.\n",
    "10. `PCT_WATER_TOT`: quantitative data inciduating the percentage of water area.\n",
    "11. `PCT_WATER_INLAND`: quantitative data indicating the percentage of inland water area in the state.\n",
    "\n",
    "Overall, I had 4 pieces of nominal data, 1 pieces of ordinal data, and 6 pieces of ordinal data. I performed the necessary encodings by using a one hot encoder (`OneHotEncoder(drop='first', handle_unknown='ignore')`) for the nominal values and an ordinal encoder (`OrdinalEncoder(categories=[<categories>], dtype=int)`) for the ordinal column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - define functions for imputation of null data. This is to ensure that the model can be fit without errors due to missing data.\n",
    "\n",
    "def prob_impute_categorical(series): # function to impute missing categorical values based on observed distribution\n",
    "    series = series.copy() # create a copy of the series to avoid modifying the original\n",
    "    num_null = series.isnull().sum() # count number of null values\n",
    "    fill_values = np.random.choice(series.dropna(), size=num_null, replace=True) # sample from non-null values\n",
    "    series[series.isnull()] = fill_values # fill null values with sampled values\n",
    "    return series\n",
    "\n",
    "def prob_impute_numerical(series): # function to impute missing numerical values with mean\n",
    "    return series.fillna(series.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Step 2 - X data preparation. Get columns to be used in the model and impute any missing values for ordinal columns\n",
    "climate_relevant_columns = ['POSTAL.CODE', 'NERC.REGION', 'MONTH', 'YEAR',\n",
    "                    'CLIMATE.REGION', 'ANOMALY.LEVEL', 'CLIMATE.CATEGORY',\n",
    "                    'HURRICANE.NAMES', 'PCT_LAND', 'PCT_WATER_TOT', 'PCT_WATER_INLAND'] # columns to use for modeling\n",
    "model_data = data[climate_relevant_columns] # subset data to only model columns\n",
    "\n",
    "# Impute missing values\n",
    "for col in model_data.columns:\n",
    "    if model_data[col].dtype == 'object': # if column is categorical\n",
    "        model_data[col] = prob_impute_categorical(model_data[col]) # impute using categorical imputation\n",
    "    else: # if column is numerical\n",
    "        model_data[col] = prob_impute_numerical(model_data[col]) # impute using numerical imputation\n",
    "\n",
    "# Step 3 - y data preparation - cause of the outage\n",
    "y = data['CAUSE.CATEGORY']\n",
    "\n",
    "# Step 4 - create preprocessing and modeling pipeline\n",
    "col_trans = make_column_transformer( # create column transformer for preprocessing\n",
    "    (OneHotEncoder(drop='first', handle_unknown='ignore'), ['POSTAL.CODE', 'NERC.REGION', 'CLIMATE.REGION', 'HURRICANE.NAMES']), # one-hot encode nominal categorical columns\n",
    "    (OrdinalEncoder(categories=[['cold', 'normal', 'warm']], dtype=int), ['CLIMATE.CATEGORY']), # ordinal encode the CLIMATE.CATEGORY column because it has an inherent order to it\n",
    "    remainder='passthrough', # pass through any remaining columns without transformation\n",
    ")\n",
    "\n",
    "pipeline = make_pipeline( # create pipeline with preprocessing and decision tree classifier model\n",
    "    col_trans,\n",
    "    DecisionTreeClassifier()\n",
    ")\n",
    "\n",
    "# Step 5 - split data into training and testing sets\n",
    "X = model_data\n",
    "y = y # target variable defined above\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5798045602605864\n",
      "Saved figure to model_evaluation\\decision_tree_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "# Step 6 - fit the pipeline and evaluate the model\n",
    "pipeline.fit(X_train, y_train) # fit the pipeline on training data\n",
    "\n",
    "accuracy = (pipeline.predict(X_test) == y_test).mean() # compute accuracy on test data of the pipeline, which includes preprocessing and model prediction\n",
    "print(f'Accuracy: {accuracy}') # print accuracy\n",
    "cm = confusion_matrix(y_test, pipeline.predict(X_test), labels=pipeline.classes_) # compute confusion matrix\n",
    "fig = px.imshow(\n",
    "    cm,\n",
    "    text_auto=True,\n",
    "    labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "    x=pipeline.classes_,\n",
    "    y=pipeline.classes_,\n",
    "    color_continuous_scale='Blues'\n",
    ") # create heatmap of confusion matrix\n",
    "fig.update_layout(title=\"Decision Tree Classifier Confusion Matrix\") # update layout of figure to have a title\n",
    "save_figure(fig, 'decision_tree_confusion_matrix', save_folder='model_evaluation') # save the confusion matrix figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected random accuracy: 0.334\n"
     ]
    }
   ],
   "source": [
    "# Calculate class proportions for CAUSE.CATEGORY\n",
    "class_counts = data['CAUSE.CATEGORY'].value_counts(normalize=True)\n",
    "# Expected accuracy for random selection proportional to class distribution\n",
    "expected_random_accuracy = sum(class_counts ** 2)\n",
    "\n",
    "print(f\"Expected random accuracy: {expected_random_accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, I was able to set my model accuracy at ~57.9% (this willl vary based on each run), with the following confusion matrix.\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"model_evaluation/decision_tree_confusion_matrix.png\" width=\"600\">\n",
    "</div>\n",
    "\n",
    "Evaluating my current model, I do not believe that it is good. This is because of a few reasons:\n",
    "1. There are some columns that can cause multicollinearity. Specifically, since you can predict `PCT_WATER_TOT = 1 - PCT_LAND`, the mdoel is multicollinear.\n",
    "2. I can make current features more useful by transforming them. \n",
    "   1. Currently I impute values into `HURRICANE.NAMES`, but since most outages aren't related to hurricanes, this is a misrepresentative column when making predictions. It could be made better by a `Binarizer`.\n",
    "   2. I also simply use the quantitative `MONTH` values. However, it might be more useful to encode these as seasons after a mapping, to generalize the model better and make it a nominal categorical variable. One issue with quantitative months is that numerically far months in winter (ex. December, January), might actually want to be adjacent to each other.\n",
    "3. Finally, I believe my model is not good because currently it sits at around a 57% accuracy, which although is better than the expected random accuracy of 33.4%, seems like it can be significantly improved to ensure that policy/mitigation actions ar taken with more confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model improves on the baseline model. Specifically, I performed the following for the readability of the model:\n",
    "1. I applied a `StandardScaler` on my quantitative columns. Although this does not specifically help with model accuracy or changes how decisions are made, it does make the model more interpretable.\n",
    "2. I removed columns that are highly correlated to reduce model multicollinarity. The column that this was relevant to was `PCT_WATER_TOT` and `PCT_LAND`, as you could calculate `PCT_WATER_TOT = 1 - PCT_LAND`.\n",
    "   \n",
    "In terms of new features, I added the following:\n",
    "1. I **binarized** the `HURRICANE.NAMES` column to be `HURRICANE_PRESENT`. This is because a) imputing `NaN` values with randomly selected hurricane names is misrepresentative of the climate conditions leading up to the power outage and b) the presence or absence of a hurricane is a relevant piece of data that can be used to predict the cause of an outage, especially considering its impact on climate data at that specific time point.\n",
    "2. I **mapped** the quantitative column of `MONTH` to a categorical column of `SEASON`. This is because a) encoding the `SEASON` as opposed to `MONTH` allows for better generalization to make our predictions (i.e. whether a month is June, July, or August might be less relevant than the fact that these are summer months and therefore have specific climate characteristics) and b) numerically far months (like December and January) are able to be categorically adjacent to one another and therefore more representative of climate patterns.\n",
    "   \n",
    "From there, I tested a second model (**Random Forest**) and used **GridSearchCV** with both models in order to determine the best-performing model and set of hyperparameters.\n",
    "\n",
    "For the **Decision Tree** model, I made sure to optimize for the following hyperparameters:\n",
    "1. `max_depth`: this controls the maximum depth of the tree, and I hope to prevent overfitting by limiting it and finding the optimal depth for maximum accuracy.\n",
    "2. `min_samples_split`: this controls the minimum number of samples required to split an internal node, and I hope to prevent overfitting by requiring a higher number of samples to make splits.\n",
    "\n",
    "For the **Random Forest** model, I made sure to optimize for the same hyperparameters as the decision tree plus one more:\n",
    "1. `n_estimators`: this controls how MANY decision trees I am using to make the general decision. If I have too few, it can lead to unstable predictions, but too many can lead to unnecessary computation and no gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `modify_data_for_modeling` function\n",
    "\n",
    "In the interest of making data preprocessing simpler in Step 8, where I perform a fairness analysis, I made a function for the initial data modification that needs to happen for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_data_for_modeling(data):\n",
    "    # Step 1 - re-select relevant columns for climate modeling\n",
    "    climate_relevant_columns = ['YEAR', 'MONTH', 'POSTAL.CODE', 'NERC.REGION',\n",
    "                        'CLIMATE.REGION', 'ANOMALY.LEVEL', 'CLIMATE.CATEGORY',\n",
    "                        'HURRICANE.NAMES', 'PCT_LAND', 'PCT_WATER_TOT', 'PCT_WATER_INLAND']\n",
    "    model_data_final = data[climate_relevant_columns] # subset data to only model columns\n",
    "\n",
    "    # Step 2 - analyze numerical columns and decide which ones to remove based on correlation\n",
    "    numerical_cols = model_data_final.select_dtypes(include=['number']).columns.tolist() # select all numerical columns \n",
    "    correlation_matrix = model_data_final[numerical_cols].corr().abs() # compute absolute correlation matrix\n",
    "    # It is shown that PCT_LAND has a high correlation with PCT_WATER_TOT (100 - PCT_LAND), so we will remove that column\n",
    "    model_data_final = model_data_final.drop(columns=['PCT_WATER_TOT'])\n",
    "\n",
    "    # # Step 3 - binarize the HURRICANE.NAMES column\n",
    "    # The name of a hurricane does not provide useful information, but whether or not there was a hurricane does\n",
    "    model_data_final['HURRICANE_PRESENT'] = model_data_final['HURRICANE.NAMES'].notnull().astype(int) # create binary feature for hurricane presence\n",
    "    model_data_final = model_data_final.drop(columns=['HURRICANE.NAMES']) # drop the original\n",
    "\n",
    "    # Step 4 - convert MONTH to categorical type using seasons\n",
    "    def month_to_season(month):\n",
    "        if month in [12, 1, 2]:\n",
    "            return 'Winter'\n",
    "        elif month in [3, 4, 5]:\n",
    "            return 'Spring'\n",
    "        elif month in [6, 7, 8]:\n",
    "            return 'Summer'\n",
    "        elif month in [9, 10, 11]:\n",
    "            return 'Fall'\n",
    "    model_data_final['SEASON'] = model_data_final['MONTH'].apply(month_to_season) # map month to season\n",
    "    model_data_final = model_data_final.drop(columns=['MONTH']) # drop the original MONTH column\n",
    "\n",
    "    # Step 5 - impute missing values using previously defined functions\n",
    "    for col in model_data_final.columns:\n",
    "        if col in ['POSTAL.CODE', 'NERC.REGION', 'CLIMATE.REGION', 'CLIMATE.CATEGORY', 'SEASON']: # if column is categorical\n",
    "            model_data_final[col] = prob_impute_categorical(model_data_final[col]) # impute using categorical imputation\n",
    "        elif col in ['YEAR', 'ANOMALY.LEVEL', 'PCT_LAND', 'PCT_WATER_INLAND']: # if column is numerical\n",
    "            model_data_final[col] = prob_impute_numerical(model_data_final[col]) # impute using numerical imputation \n",
    "            \n",
    "    return model_data_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - re-prepare X and y data for modeling with modified function\n",
    "X = modify_data_for_modeling(data) # features\n",
    "y = data['CAUSE.CATEGORY'] # target variable\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - create preprocessing and modeling pipelines for decision tree and random forest classifiers\n",
    "col_trans = make_column_transformer( # create column transformer for preprocessing\n",
    "    (OneHotEncoder(drop='first', handle_unknown='ignore'), ['POSTAL.CODE', 'NERC.REGION', 'CLIMATE.REGION', 'HURRICANE_PRESENT', 'SEASON']), # one-hot encode nominal categorical columns\n",
    "    (OrdinalEncoder(categories=[['cold', 'normal', 'warm']], dtype=int), ['CLIMATE.CATEGORY']), # ordinal encode the CLIMATE.CATEGORY column because it has an inherent order to it\n",
    "    (StandardScaler(), ['YEAR', 'ANOMALY.LEVEL', 'PCT_LAND', 'PCT_WATER_INLAND']), # standard scale numerical columns\n",
    "    remainder='passthrough', # pass through any remaining columns without transformation\n",
    ")\n",
    "\n",
    "pipeline_dt = make_pipeline( # create pipeline with preprocessing and decision tree classifier model\n",
    "    col_trans,\n",
    "    DecisionTreeClassifier()\n",
    ")\n",
    "\n",
    "pipeline_rf = make_pipeline( # create pipeline with preprocessing and random forest classifier model\n",
    "    col_trans,\n",
    "    RandomForestClassifier()\n",
    ")\n",
    "\n",
    "# Step 3 - perform grid search cross-validation to find best hyperparameters for both models\n",
    "param_grid_dt = {\n",
    "    'decisiontreeclassifier__max_depth': list(range(5, 45)) + [None],\n",
    "    'decisiontreeclassifier__min_samples_split': list(range(2, 10)),\n",
    "}\n",
    "\n",
    "param_grid_rf = {\n",
    "    'randomforestclassifier__n_estimators': list(range(10, 110, 10)),\n",
    "    'randomforestclassifier__max_depth': list(range(5, 45)) + [None],\n",
    "    'randomforestclassifier__min_samples_split': list(range(2, 10)),\n",
    "}\n",
    "\n",
    "dt_grid_search = GridSearchCV(\n",
    "    pipeline_dt,\n",
    "    param_grid=param_grid_dt,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    ")\n",
    "\n",
    "rf_grid_search = GridSearchCV(\n",
    "    pipeline_rf,\n",
    "    param_grid=param_grid_rf,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit_and_evaluate_model` function\n",
    "\n",
    "In order to ensure that we don't have to repeat code for the Random Forest and Decision Trees grid searches, I set up this function to evaluate and plot the confusion matrices of both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_evaluate_model(grid_search, model_name, X_train, y_train, X_test, y_test): # function to fit and evaluate model\n",
    "    grid_search.fit(X_train, y_train) # fit the grid search on training data\n",
    "    best_model = grid_search.best_estimator_ # get the best model from grid search\n",
    "    accuracy = (best_model.predict(X_test) == y_test).mean() # calculate accuracy on test data\n",
    "    print(\"Model: \", model_name) # print model name\n",
    "    print(f'Best Parameters: {grid_search.best_params_}') # print best hyperparameters\n",
    "    print(f'Accuracy: {accuracy}') # print accuracy\n",
    "    fig = px.imshow(\n",
    "        cm,\n",
    "        text_auto=True,\n",
    "        labels=dict(x=\"Predicted\", y=\"Actual\", color=\"Count\"),\n",
    "        x=pipeline.classes_,\n",
    "        y=pipeline.classes_,\n",
    "        color_continuous_scale='Blues'\n",
    "    ) # create heatmap of confusion matrix\n",
    "    fig.update_layout(title=f\"{model_name} Confusion Matrix\") # update layout of figure to have a title\n",
    "    save_figure(fig, f'{model_name.lower().replace(\" \", \"_\")}_confusion_matrix', save_folder='model_evaluation') # save the confusion matrix figure\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model:  Decision Tree Grid Search\n",
      "Best Parameters: {'decisiontreeclassifier__max_depth': 11, 'decisiontreeclassifier__min_samples_split': 2}\n",
      "Accuracy: 0.6156351791530945\n",
      "Saved figure to model_evaluation\\decision_tree_grid_search_confusion_matrix.png\n",
      "Model:  Random Forest Grid Search\n",
      "Best Parameters: {'randomforestclassifier__max_depth': 42, 'randomforestclassifier__min_samples_split': 5, 'randomforestclassifier__n_estimators': 70}\n",
      "Accuracy: 0.6742671009771987\n",
      "Saved figure to model_evaluation\\random_forest_grid_search_confusion_matrix.png\n"
     ]
    }
   ],
   "source": [
    "best_dt_model = fit_and_evaluate_model(dt_grid_search, \"Decision Tree Grid Search\", X_train, y_train, X_test, y_test) # fit and evaluate decision tree model\n",
    "best_rf_model = fit_and_evaluate_model(rf_grid_search, \"Random Forest Grid Search\", X_train, y_train, X_test, y_test) # fit and evaluate random forest model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the fairness analysis, I chose to measure whether my model would perform better or worse for individuals who were in areas where a greater median percentage of their population was rural compared to areas where a greater median percentage of the population is urban. I theorize that **urban populations** will have a lower accuracy due to the diverse causes of outages/incidents correlated with the large population, whereas rural populations have fewer, more predictable causes.\n",
    "\n",
    "The reason I used `POPDEN_RURAL` instead of `AREAPCT_URBAN` to binarize is because cities are usually very dense, leading to the majority of land in the U.S. to be rural. Therefore there is less variation (and therefore a worse split) in `AREAPCT_URBAN` as compared to `POPDEN_RURAL`, which while also might tend to be right skewed, does tend itself to an increased variance and therefore a more valid binarization. This can be verified using the univariate plots of the two columns:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"univariate_analysis/AREAPCT_URBAN.png\" width=\"400\">\n",
    "    <img src=\"univariate_analysis/POPDEN_RURAL.png\" width=\"400\">\n",
    "</div>\n",
    "\n",
    "In order to binarize our `POPDEN_RURAL` column into \"rural\" and \"urban\", I split them with the median of that column as the binarization threshold. Then, I formed the test:\n",
    "1. **Null**: our model is fair. The accuracy for \"rural\" and \"urban\" population is roughly the same, and any differences are due to random chance.\n",
    "2. **Alternative**: our model is unfair. The accuracy for \"urban\" populations is lower than the accuracy for \"rural\" populations.\n",
    "3. **Test statistic**: the signed difference in accuracy between \"urban\" and \"rural\" populations.\n",
    "\n",
    "The significance level was kept at 0.05, the industry standard for these tests, as there is no compelling reason to make the test more or less selective. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-value: 0.071\n",
      "Saved figure to fairness_analysis\\fairness_permutation_test_popden_rural_signed_accuracy_difference.png\n"
     ]
    }
   ],
   "source": [
    "# Create a copy of the data to prevent modifying the original\n",
    "data_binarized = data.copy()\n",
    "\n",
    "# Step 1 - define the statistic to binarize\n",
    "binarization_threshold = data['POPDEN_RURAL'].median()\n",
    "data_binarized['POPDEN_RURAL'] = (data_binarized['POPDEN_RURAL'] > binarization_threshold).astype(int)\n",
    "\n",
    "# Step 2 - define function to calculate recall TVD between rural and urban areas\n",
    "def calculate_signed_acc_difference_rural_urban(best_model, data):\n",
    "    # Step 2.1 - define X and y for modeling\n",
    "    data_rural = data[data['POPDEN_RURAL'] == 1]\n",
    "    X_rural = modify_data_for_modeling(data_rural)\n",
    "    y_rural = data_rural['CAUSE.CATEGORY']\n",
    "\n",
    "    data_urban = data[data['POPDEN_RURAL'] == 0]\n",
    "    X_urban = modify_data_for_modeling(data_urban)\n",
    "    y_urban = data_urban['CAUSE.CATEGORY']\n",
    "\n",
    "    # Step 2.2 - split data into training and testing sets for rural and urban\n",
    "    X_rural_train, X_rural_test, y_rural_train, y_rural_test = train_test_split(X_rural, y_rural, test_size=0.2)\n",
    "    X_urban_train, X_urban_test, y_urban_train, y_urban_test = train_test_split(X_urban, y_urban, test_size=0.2)\n",
    "\n",
    "    # Step 2.3 - create and fit the model pipelines for rural and urban\n",
    "    best_model.fit(X_rural_train, y_rural_train)\n",
    "    predictions_rural = best_model.predict(X_rural_test)\n",
    "    accuracy_rural = (predictions_rural == y_rural_test).mean()\n",
    "\n",
    "    best_model.fit(X_urban_train, y_urban_train)\n",
    "    predictions_urban = best_model.predict(X_urban_test)\n",
    "    accuracy_urban = (predictions_urban == y_urban_test).mean()\n",
    "\n",
    "    # Step 2.4 - calculate and return the TVD between rural and urban recall distributions\n",
    "    return accuracy_rural - accuracy_urban\n",
    "\n",
    "# Step 3 - define the observed statistic\n",
    "observed_statistic = calculate_signed_acc_difference_rural_urban(best_rf_model, data_binarized)\n",
    "\n",
    "# Step 4 - perform permutation test\n",
    "n_repetitions = 1000\n",
    "permuted_tvds = []\n",
    "for _ in range(n_repetitions):\n",
    "    # Step 4.1 - Shuffle the POPDEN_RURAL column to break association\n",
    "    data_shuffled = data_binarized.copy()\n",
    "    data_shuffled['POPDEN_RURAL'] = np.random.permutation(data_shuffled['POPDEN_RURAL'])\n",
    "    \n",
    "    # Step 4.2 - Calculate the recall TVD for the shuffled data\n",
    "    tvd = calculate_signed_acc_difference_rural_urban(best_dt_model, data_shuffled)\n",
    "    permuted_tvds.append(tvd)\n",
    "\n",
    "# Step 5 - graph the permutation test distribution\n",
    "p_value = calculate_p_value(observed_statistic, permuted_tvds)\n",
    "print(f'P-value: {p_value}')\n",
    "fig = plot_permutation_test_distribution(permuted_tvds, observed_statistic, 'POPDEN_RURAL', 'Signed Accuracy Difference', p_value=p_value, title='Empirical Distribution of the Signed Accuracy Difference between <br> Rural and Urban Areas as Predicted by the Best Random Forest Model')\n",
    "save_figure(fig, 'fairness_permutation_test_popden_rural_signed_accuracy_difference', save_folder='fairness_analysis') # save the figure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through this, we were able to determine that the resulting p-value was 0.26, implying that I **fail to reject the null hypothesis** and therefore cannot reliably state that the signed accuracy difference between rural and urban areas is significant enough to prove urban areas have an overall lower accuracy. Here is the figure from the permutation test:\n",
    "\n",
    "<div style=\"display: flex; justify-content: center; gap: 20px;\">\n",
    "    <img src=\"fairness_analysis\\fairness_permutation_test_popden_rural_signed_accuracy_difference.png\" width=\"600\">\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsc80",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
